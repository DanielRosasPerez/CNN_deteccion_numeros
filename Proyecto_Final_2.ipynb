{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importamos los datos para nuestro modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clases = list()\n",
    "Imagenes_por_Clase = list()\n",
    "\n",
    "for clase in range(0,len(os.listdir(\"myData\"))):\n",
    "    \n",
    "    ruta = os.path.join(\"myData\",str(clase))\n",
    "    \n",
    "    for imagen in os.listdir(ruta):\n",
    "        \n",
    "        img = cv2.imread(os.path.join(ruta,imagen), cv2.IMREAD_GRAYSCALE) # Recuperamos cada imagen en escala de grises.\n",
    "        img = cv2.resize(img, (32,32))\n",
    "        Imagenes_por_Clase.append(img)\n",
    "        Clases.append(clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10160\n",
      "10160\n"
     ]
    }
   ],
   "source": [
    " # En total hay 10 clases (del 0 al 9). Sin embargo, con la finalidad de revolver los datos, asociamos cada muestra con su respectiva etiqueta:\n",
    "print(len(Clases))\n",
    "print(len(Imagenes_por_Clase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertimos nuestras listas en arreglos para poder usar numpy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagenes_por_Clase = np.asarray(Imagenes_por_Clase)\n",
    "Clases = np.asarray(Clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10160, 32, 32)\n",
      "(10160,)\n"
     ]
    }
   ],
   "source": [
    "print(Imagenes_por_Clase.shape)\n",
    "print(Clases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1016, 1: 1016, 2: 1016, 3: 1016, 4: 1016, 5: 1016, 6: 1016, 7: 1016, 8: 1016, 9: 1016})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Cantidad_Imagenes_por_Clase = Counter(Clases)\n",
    "print(Cantidad_Imagenes_por_Clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, por cada clase hay **1016** muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dividimos los datos de entrenamiento:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagenes_Entrenamiento, Imagenes_Testeo, Etiquetas_Entrenamiento, Etiquetas_Testeo = train_test_split(Imagenes_por_Clase,\n",
    "                                                                                                     Clases, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagenes_Entrenamiento, Imagenes_Validacion, Etiquetas_Entrenamiento, Etiquetas_Validacion = train_test_split(Imagenes_Entrenamiento,\n",
    "                                                                                                             Etiquetas_Entrenamiento,\n",
    "                                                                                                             test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6502, 32, 32)\n",
      "(6502,)\n",
      "\n",
      "(1626, 32, 32)\n",
      "(1626,)\n"
     ]
    }
   ],
   "source": [
    "print(Imagenes_Entrenamiento.shape)\n",
    "print(Etiquetas_Entrenamiento.shape)\n",
    "############\n",
    "print('\\r')\n",
    "############\n",
    "print(Imagenes_Validacion.shape)\n",
    "print(Etiquetas_Validacion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2032, 32, 32)\n",
      "(2032,)\n"
     ]
    }
   ],
   "source": [
    "print(Imagenes_Testeo.shape)\n",
    "print(Etiquetas_Testeo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29701394ac8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Imagenes_Entrenamiento[10], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamos nuestros datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(etiquetas):\n",
    "    \"\"\"Esta función nos arroja un vector one_hot para cada etiqueta.\"\"\"\n",
    "    \n",
    "    cantidad_clases = len(set(etiquetas))\n",
    "    vectores_OneHot = np.zeros(shape=(etiquetas.shape[0], cantidad_clases), dtype=\"float64\")\n",
    "    \n",
    "    for i in range(vectores_OneHot.shape[0]):\n",
    "        vectores_OneHot[i,etiquetas[i]] = 1\n",
    "    return vectores_OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagenes_Entrenamiento.resize((6502,32,32,1))\n",
    "Imagenes_Validacion.resize((1626,32,32,1))\n",
    "Imagenes_Testeo.resize((2032,32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagenes_Entrenamiento = Imagenes_Entrenamiento/255\n",
    "Imagenes_Validacion = Imagenes_Validacion/255\n",
    "Imagenes_Testeo = Imagenes_Testeo/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Etiquetas_Entrenamiento = one_hot_encode(Etiquetas_Entrenamiento)\n",
    "Etiquetas_Validacion = one_hot_encode(Etiquetas_Validacion)\n",
    "Etiquetas_Testeo = one_hot_encode(Etiquetas_Testeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debido a que nuestra cantidad de datos es pequeña, haremos uso de la técnica conocida como \"Aumento de Datos\":**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        )\n",
    "\n",
    "train_datagen.fit(Imagenes_Entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos nuestra red neuronal:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6502 samples, validate on 1626 samples\n",
      "Epoch 1/10\n",
      "6502/6502 [==============================] - 4s 689us/step - loss: 0.9359 - accuracy: 0.6904 - val_loss: 0.2130 - val_accuracy: 0.9360\n",
      "Epoch 2/10\n",
      "6502/6502 [==============================] - 4s 642us/step - loss: 0.1837 - accuracy: 0.9416 - val_loss: 0.1037 - val_accuracy: 0.9656\n",
      "Epoch 3/10\n",
      "6502/6502 [==============================] - 4s 654us/step - loss: 0.1121 - accuracy: 0.9609 - val_loss: 0.0677 - val_accuracy: 0.9766\n",
      "Epoch 4/10\n",
      "6502/6502 [==============================] - 4s 671us/step - loss: 0.0842 - accuracy: 0.9725 - val_loss: 0.0479 - val_accuracy: 0.9859\n",
      "Epoch 5/10\n",
      "6502/6502 [==============================] - 4s 631us/step - loss: 0.0675 - accuracy: 0.9791 - val_loss: 0.0428 - val_accuracy: 0.9859\n",
      "Epoch 6/10\n",
      "6502/6502 [==============================] - 4s 638us/step - loss: 0.0474 - accuracy: 0.9859 - val_loss: 0.0420 - val_accuracy: 0.9852\n",
      "Epoch 7/10\n",
      "6502/6502 [==============================] - 4s 677us/step - loss: 0.0417 - accuracy: 0.9868 - val_loss: 0.0344 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "6502/6502 [==============================] - 4s 656us/step - loss: 0.0408 - accuracy: 0.9903 - val_loss: 0.0320 - val_accuracy: 0.9895\n",
      "Epoch 9/10\n",
      "6502/6502 [==============================] - 4s 632us/step - loss: 0.0293 - accuracy: 0.9909 - val_loss: 0.0429 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "6502/6502 [==============================] - 4s 633us/step - loss: 0.0313 - accuracy: 0.9905 - val_loss: 0.0274 - val_accuracy: 0.9908\n",
      "Tiempo total de entrenamiento: 43.1705 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(32,32,1), strides=(1,1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "BatchNormalization()\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "BatchNormalization()\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "BatchNormalization()\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "BatchNormalization()\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "##################################\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "##################################\n",
    "from timeit import default_timer\n",
    "\n",
    "start_time = default_timer()\n",
    "\n",
    "\n",
    "model.fit(Imagenes_Entrenamiento, Etiquetas_Entrenamiento, epochs=10, batch_size=64, \n",
    "          validation_data=(Imagenes_Validacion, Etiquetas_Validacion))\n",
    "\"\"\"\n",
    "\n",
    "history = model.fit_generator(train_datagen.flow(Imagenes_Entrenamiento, Etiquetas_Entrenamiento, batch_size=64),\n",
    "                             steps_per_epoch=3000,\n",
    "                             epochs=10,\n",
    "                             validation_data=(Imagenes_Validacion, Etiquetas_Validacion),\n",
    "                             shuffle=1\n",
    "                             )\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Tiempo total de entrenamiento: {np.around(default_timer() - start_time, decimals=4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032/2032 [==============================] - 0s 179us/step\n",
      "Pérdida: 0.013834684189489068\n",
      "Precisión: 0.9955708384513855\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(Imagenes_Testeo, Etiquetas_Testeo)\n",
    "print(f\"Pérdida: {loss}\")\n",
    "print(f\"Precisión: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Modelo_PFinal2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"Modelo_PFinal2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "########### PARAMETERS ##############\n",
    "width = 640\n",
    "height = 480\n",
    "threshold = 0.65 # MINIMUM PROBABILITY TO CLASSIFY\n",
    "cameraNo = 0\n",
    "#####################################\n",
    "\n",
    "#### CREATE CAMERA OBJECT\n",
    "cap = cv2.VideoCapture(cameraNo)\n",
    "cap.set(3,width)\n",
    "cap.set(4,height)\n",
    "\n",
    "#### LOAD THE TRAINNED MODEL \n",
    "\n",
    "\n",
    "#### PREPORCESSING FUNCTION\n",
    "def preProcessing(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "while True:\n",
    "    success, imgOriginal = cap.read()\n",
    "    img = np.asarray(imgOriginal)\n",
    "    img = cv2.resize(img,(32,32))\n",
    "    img = preProcessing(img)\n",
    "    cv2.imshow(\"Processsed Image\",img)\n",
    "    img = img.reshape(1,32,32,1)\n",
    "    #### PREDICT\n",
    "    classIndex = int(model.predict_classes(img))\n",
    "    #print(classIndex)\n",
    "    predictions = model.predict(img)\n",
    "    #print(predictions)\n",
    "    probVal= np.amax(predictions)\n",
    "    #print(classIndex,probVal)\n",
    "\n",
    "    if probVal> threshold:\n",
    "        cv2.putText(imgOriginal,str(classIndex) + \"   \"+str(probVal),\n",
    "                    (50,50),cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    1,(0,0,255),1)\n",
    "\n",
    "    cv2.imshow(\"Original Image\",imgOriginal)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
